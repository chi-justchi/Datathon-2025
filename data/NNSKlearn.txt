import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import roc_auc_score, average_precision_score
import matplotlib.pyplot as plt

# -----------------------------
# 1) Parse lists -> matrices
# -----------------------------
def stack_from_samples(samples: List[Dict]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Turn [{'emb':(D,), 'lab':(L,), ...}, ...] into:
      X : (N, D)
      Y : (N, L)
      ids: (N,) patient ids
    """
    if len(samples) == 0:
        raise ValueError("Empty sample list.")
    # collect embeddings and labels
    X_list, Y_list, ids = [], [], []
    for s in samples:
        emb = np.asarray(s["emb"])
        if emb.ndim > 1:
            emb = emb.reshape(-1)  # flatten just in case
        X_list.append(emb.astype(np.float32))
        Y_list.append(np.asarray(s["lab"], dtype=np.float32))
        ids.append(int(s["patient_id"]))
    X = np.vstack(X_list)
    Y = np.vstack(Y_list)
    ids = np.asarray(ids)
    # Replace NaNs/inf if any
    X = np.nan_to_num(X, posinf=0.0, neginf=0.0)
    Y = np.nan_to_num(Y, posinf=0.0, neginf=0.0)
    # Clip labels to [0,1] and cast to int
    Y = (Y > 0.5).astype(int)
    return X, Y, ids

# ----------------------------------------------------------
# 2) Train one-vs-rest MLP on (train, valid, test) splits
# ----------------------------------------------------------
def fit_eval_mlp_on_splits(
    train_samples: List[Dict],
    valid_samples: List[Dict],
    test_samples:  List[Dict],
    label_names: List[str],
    seed: int = 42,
    hidden=(512, 256),
    max_iter=100,
) -> Dict[str, Dict[str, float]]:
    """
    Fits one MLP per label using train, early-stopping on internal val split,
    and reports metrics on VALID and TEST splits.
    Returns:
      {
        'valid': {'AUROC_macro':..., 'AP_macro':..., 'AUROC_micro':..., 'AP_micro':...},
        'test' : {...}
      }
    """
    # Parse splits
    X_tr, Y_tr, _ = stack_from_samples(train_samples)
    X_va, Y_va, _ = stack_from_samples(valid_samples)
    X_te, Y_te, _ = stack_from_samples(test_samples)

    # Standardize using TRAIN only
    scaler = StandardScaler()
    X_tr_s = scaler.fit_transform(X_tr)
    X_va_s = scaler.transform(X_va)
    X_te_s = scaler.transform(X_te)

    L = Y_tr.shape[1]
    # storage for probabilities
    P_va = np.zeros((X_va_s.shape[0], L), dtype=float)
    P_te = np.zeros((X_te_s.shape[0], L), dtype=float)

    for j in range(L):
        y_tr = Y_tr[:, j]
        # degenerate case: single class in train
        if len(np.unique(y_tr)) < 2:
            const_p = float(y_tr.mean())  # trivial prob
            P_va[:, j] = const_p
            P_te[:, j] = const_p
            continue

        # balanced sample weights
        sw = compute_sample_weight("balanced", y_tr)

        clf = MLPClassifier(
            hidden_layer_sizes=hidden,
            activation="relu",
            solver="adam",
            alpha=1e-4,
            learning_rate_init=1e-3,
            batch_size=256,
            max_iter=max_iter,
            early_stopping=True,
            n_iter_no_change=10,
            validation_fraction=0.15,
            shuffle=True,
            random_state=seed,
        )
        clf.fit(X_tr_s, y_tr, sample_weight=sw)
        # predict_proba may not exist for binary with lbfgs, but here solver='adam' supports it
        P_va[:, j] = clf.predict_proba(X_va_s)[:, 1]
        P_te[:, j] = clf.predict_proba(X_te_s)[:, 1]

    # --- helpers to compute split metrics safely ---
    def split_metrics(Y_true: np.ndarray, P: np.ndarray) -> Dict[str, float]:
        per_auc, per_ap, valid_cols = {}, {}, []
        for j, lab in enumerate(label_names):
            yt = Y_true[:, j]
            # guard: if test/valid has single class, AUROC/AP undefined
            if len(np.unique(yt)) < 2:
                per_auc[lab] = np.nan
                per_ap[lab] = np.nan
            else:
                per_auc[lab] = roc_auc_score(yt, P[:, j])
                per_ap[lab] = average_precision_score(yt, P[:, j])
                valid_cols.append(j)

        auroc_macro = float(np.nanmean(list(per_auc.values()))) if per_auc else np.nan
        ap_macro    = float(np.nanmean(list(per_ap.values())))  if per_ap else np.nan
        if len(valid_cols):
            auroc_micro = roc_auc_score(Y_true[:, valid_cols], P[:, valid_cols], average="micro")
            ap_micro    = average_precision_score(Y_true[:, valid_cols], P[:, valid_cols], average="micro")
        else:
            auroc_micro = np.nan
            ap_micro    = np.nan

        return dict(
            AUROC_macro=auroc_macro,
            AP_macro=ap_macro,
            AUROC_micro=auroc_micro,
            AP_micro=ap_micro,
            per_label_AUROC=per_auc,
            per_label_AP=per_ap,
        )

    out = {
        "valid": split_metrics(Y_va, P_va),
        "test":  split_metrics(Y_te, P_te),
    }
    return out

# ----------------------------------------------------------
# 3) Convenience: run end-to-end on your three splits
# ----------------------------------------------------------
# Your pathologies from the dataset (after Pleural Effusion -> Effusion)
label_cols = [
    "Enlarged Cardiomediastinum","Cardiomegaly","Lung Opacity","Lung Lesion","Edema",
    "Consolidation","Pneumonia","Atelectasis","Pneumothorax","Effusion",
    "Pleural Other","Fracture","Support Devices"
]

# Example: train a BiomedCLIP head using your prepared lists
biomed_results = fit_eval_mlp_on_splits(
    train_samples=biomedclip_train_dataset,
    valid_samples=biomedclip_valid_dataset,
    test_samples=biomedclip_test_dataset,
    label_names=label_cols,
    seed=42,
    hidden=(512,256),
    max_iter=120,
)

print("\n[BiomedCLIP] VALID:",
      {k: round(v,4) for k,v in biomed_results["valid"].items() if not isinstance(v, dict)})
print("[BiomedCLIP] TEST:",
      {k: round(v,4) for k,v in biomed_results["test"].items() if not isinstance(v, dict)})

# ----------------------------------------------------------
# 4) (Optional) Heatmap helper if you run multiple FMs
# ----------------------------------------------------------
def plot_per_label_heatmap(per_label_results: Dict[str, Dict[str, float]], label_cols: List[str], title="Per-label AUROC by FM"):
    # per_label_results: {"BiomedCLIP": {"Atelectasis":0.75,...}, "RAD-DINO": {...}}
    df_scores = pd.DataFrame(per_label_results).T
    # ensure consistent label order
    for c in label_cols:
        if c not in df_scores.columns:
            df_scores[c] = np.nan
    df_scores = df_scores[label_cols]

    plt.figure(figsize=(14, 6))
    im = plt.imshow(df_scores.to_numpy(dtype=float), aspect="auto")
    plt.colorbar(im, label="AUROC")
    plt.yticks(range(df_scores.shape[0]), df_scores.index)
    plt.xticks(range(len(label_cols)), label_cols, rotation=45, ha="right")
    plt.title(title)
    plt.tight_layout()
    plt.show()



# Collect per-label AUROC results for BiomedCLIP
per_label_results = {
    "BiomedCLIP": biomed_results["test"]["per_label_AUROC"]
}

# Plot the heatmap
plot_per_label_heatmap(per_label_results, label_cols, 
                       title="Per-label AUROC (BiomedCLIP, Test set)")




import seaborn as sns
import matplotlib.pyplot as plt

# Grab the macro/micro metrics from the test split
summary = biomed_results["test"]
metrics = {k: v for k,v in summary.items() if not isinstance(v, dict)}

# Convert to DataFrame for plotting
df_metrics = pd.DataFrame(list(metrics.items()), columns=["Metric","Score"])

plt.figure(figsize=(6,4))
sns.barplot(data=df_metrics, x="Metric", y="Score", palette="viridis")
plt.title("BiomedCLIP â€“ Test Macro/Micro Metrics")
plt.ylabel("Score")
plt.xticks(rotation=45, ha="right")
plt.ylim(0,1)
plt.tight_layout()
plt.show()ss