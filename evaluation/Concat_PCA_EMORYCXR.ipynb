{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af8ddfa-43c3-4c21-b4a1-0c5aa5c32f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598dc7e9-0aa1-4881-9269-1cc4ab91bfaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modified from https://github.com/mlmed/torchxrayvision/blob/master/torchxrayvision/datasets.py\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from toolz import *\n",
    "# from toolz.curried import *\n",
    "# from toolz.curried.operator import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmoryDataset(Dataset):\n",
    "\n",
    "    split_ratio = [0.6, 0.2, 0.2]\n",
    "\n",
    "    embed_prefix = \"embds\"\n",
    "\n",
    "    pathologies = [\"Enlarged Cardiomediastinum\",\n",
    "                   \"Cardiomegaly\",\n",
    "                   \"Lung Opacity\",\n",
    "                   \"Lung Lesion\",\n",
    "                   \"Edema\",\n",
    "                   \"Consolidation\",\n",
    "                   \"Pneumonia\",\n",
    "                   \"Atelectasis\",\n",
    "                   \"Pneumothorax\",\n",
    "                   \"Pleural Effusion\",\n",
    "                   \"Pleural Other\",\n",
    "                   \"Fracture\",\n",
    "                   \"Support Devices\"]\n",
    "\n",
    "\n",
    "    embedding_d = {\n",
    "        \"BiomedCLIP\": Path(\"~/fsx/embeddings/EmoryCXR/embds_BiomedCLIP\"), \n",
    "        \"CheXagent\": Path(\"~/fsx/embeddings/EmoryCXR/embds_CheXagent\"), \n",
    "        \"MedGemma\": Path(\"~/fsx/embeddings/EmoryCXR/embds_MedGemma\"),\n",
    "        \"RAD-DINO\": Path(\"~/fsx/embeddings/EmoryCXR/embds_RAD-DINO\"),\n",
    "    }\n",
    "\n",
    "    csvpath = Path(\"~/fsx/embeddings/EmoryCXR/Tables/EmoryCXR_v2_FindingLabel_10162024.csv\")\n",
    "    metacsvpath = Path(\"~/fsx/embeddings/EmoryCXR/Tables/EmoryCXR_v2_metadata_08152025.csv\")\n",
    "    base_dicom_path = Path(\"/home/jupyter-oluwatunmise/fsx/embeddings/EmoryCXR/\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        views: str = [\"PA\", \"AP\"][0],\n",
    "        mode: str = [\"train\", \"validate\", \"test\"][0],\n",
    "        embedding_type: str = [\"BiomedCLIP\", \"CheXagent\", \"MedGemma\", \"RAD-DINO\", \"All\"][0],       \n",
    "        unique_patients=True,\n",
    "        seed : int = 0):\n",
    "        \n",
    "        np.random.seed(seed)  # Reset the seed so all runs are the same.        \n",
    "        self.views = views\n",
    "        self.mode = mode\n",
    "        self.embedding_type = embedding_type\n",
    "        self.unique_patients = unique_patients        \n",
    "        self.seed = seed\n",
    "        \n",
    "        self.embpath: str | list[str] = self.load_emb_path(embedding_type)\n",
    "            \n",
    "        self.csv = pd.read_csv(self.csvpath)\n",
    "        self.metacsv = pd.read_csv(self.metacsvpath)\n",
    "        self.csv = self.csv.set_index([\"AccessionNumber_anon\"])\n",
    "        self.metacsv = self.metacsv.set_index([\"AccessionNumber_anon\"])\n",
    "        self.csv = self.csv.join(self.metacsv).reset_index()   \n",
    "\n",
    "        # Keep only the desired view\n",
    "        self.csv[\"view\"] = self.csv[\"ViewPosition\"]\n",
    "        self.limit_to_selected_views(views)\n",
    "    \n",
    "        if unique_patients:\n",
    "            self.csv = self.csv.groupby(\"empi_anon\").first().reset_index()\n",
    "            \n",
    "        self.csv = self.csv.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "        self.csv = self.csv.fillna(0)\n",
    "        self.csv = self.csv[:10000]\n",
    "        \n",
    "        n_row = self.csv.shape[0]\n",
    "        \n",
    "        # spit data to one of train valid test\n",
    "        if self.mode == \"train\":\n",
    "            self.csv = self.csv[: int(n_row * self.split_ratio[0])]\n",
    "        elif self.mode == \"valid\":\n",
    "            self.csv = self.csv[\n",
    "                int(n_row * self.split_ratio[0]) : int(\n",
    "                    n_row * (self.split_ratio[0] + self.split_ratio[1])\n",
    "                )\n",
    "            ]            \n",
    "        elif self.mode == \"test\":\n",
    "            self.csv = self.csv[-int(n_row * self.split_ratio[-1]) :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"attr:mode has to be one of [train, valid, test] but your input is {self.mode}\"\n",
    "            )\n",
    "\n",
    "        # Get our classes.\n",
    "        healthy = self.csv[\"No Finding\"] == 1\n",
    "        labels = []\n",
    "        for pathology in self.pathologies:\n",
    "            if pathology in self.csv.columns:\n",
    "                self.csv.loc[healthy, pathology] = 0\n",
    "                mask = self.csv[pathology]\n",
    "\n",
    "            labels.append(mask.values)\n",
    "        self.labels = np.asarray(labels).T\n",
    "        self.labels = self.labels.astype(np.float32)\n",
    "\n",
    "        # Make all the -1 values into nans to keep things simple\n",
    "        self.labels[self.labels == -1] = 0\n",
    "\n",
    "        # Rename pathologies\n",
    "        #self.pathologies = list(np.char.replace(self.pathologies, \"Pleural Effusion\", \"Effusion\"))\n",
    "        # add consistent csv values\n",
    "\n",
    "        # patientid\n",
    "        self.csv[\"empi_anon\"] = self.csv[\"empi_anon\"].astype(str)\n",
    "\n",
    "    def __getitem__(self, i):        \n",
    "        sample = {}\n",
    "        sample[\"patient_id\"] = int(float(self.csv.iloc[i][\"empi_anon\"]))\n",
    "        sample[\"study_id\"] = int(float(self.csv.iloc[i][\"AccessionNumber_anon\"]))\n",
    "        sample[\"lab\"] = self.labels[i]\n",
    "        sample[\"emb\"] = self.load_embedding(self.csv.iloc[i][\"SOP\"])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)        \n",
    "        \n",
    "\n",
    "    def load_emb_path(self, embedding_type):\n",
    "        if self.embedding_type != \"All\":\n",
    "            return self.embedding_d[embedding_type]\n",
    "        else:            \n",
    "            return list(self.embedding_d.values())\n",
    "\n",
    "    def limit_to_selected_views(self, views):\n",
    "        \"\"\"This function is called by subclasses to filter the\n",
    "        images by view based on the values in .csv['view']\n",
    "        \"\"\"\n",
    "        if type(views) is not list:\n",
    "            views = [views]\n",
    "        if '*' in views:\n",
    "            # if you have the wildcard, the rest are irrelevant\n",
    "            views = [\"*\"]\n",
    "        self.views = views\n",
    "\n",
    "        # missing data is unknown\n",
    "        self.csv = self.csv.copy()\n",
    "        self.csv[\"view\"] = self.csv[\"view\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "        if \"*\" not in views:\n",
    "            self.csv = self.csv[self.csv[\"view\"].isin(self.views)]  # Select the view\n",
    "    \n",
    "    def load_embedding(self, embedding_id):\n",
    "        if self.embedding_type == \"All\":\n",
    "            merged_emb = []\n",
    "            for embedding_type in list(self.embedding_d.keys()):\n",
    "                emb = np.load(f\"{self.base_dicom_path/('embds_'+ embedding_type)/embedding_id}.npy\")\n",
    "                merged_emb.append(emb)\n",
    "            return np.concat(merged_emb)                \n",
    "        else:\n",
    "            return np.load(f\"{self.base_dicom_path/('embds_'+ self.embedding_type)/embedding_id}.npy\")\n",
    "\n",
    "    def load_all(self):\n",
    "        print(f\"loading all {self.mode} data\")\n",
    "        samples = []\n",
    "        for i in tqdm(range(self.__len__())):\n",
    "            sample = self.__getitem__(i)\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "\n",
    "class Dataloader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=8,\n",
    "        collate_fn=None,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        multiprocessing_context=None,\n",
    "        generator=None,\n",
    "        prefetch_factor=None,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # Initialize the parent class with all arguments\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            multiprocessing_context=multiprocessing_context,\n",
    "            generator=generator,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            persistent_workers=persistent_workers,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b0aee0-9cf0-4f83-8148-9f717a3f88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:38<00:00, 157.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 158.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 155.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================== 1) Load FM=\"All\" per split ==================\n",
    "all_train = EmoryDataset(\n",
    "    views=\"PA\", mode=\"train\", embedding_type=\"All\",\n",
    "    unique_patients=True, seed=0\n",
    ").load_all()\n",
    "\n",
    "all_valid = EmoryDataset(\n",
    "    views=\"PA\", mode=\"valid\", embedding_type=\"All\",\n",
    "    unique_patients=True, seed=0\n",
    ").load_all()\n",
    "\n",
    "all_test = EmoryDataset(\n",
    "    views=\"PA\", mode=\"test\", embedding_type=\"All\",\n",
    "    unique_patients=True, seed=0\n",
    ").load_all()\n",
    "\n",
    "# Wire into the structure your pipeline expects\n",
    "concat_splits = {\"train\": all_train, \"valid\": all_valid, \"test\": all_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddfbae-f7cb-43d1-82a4-66cef6a3229e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run PCA on Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb5ad7-3b07-47c2-94e1-0e3e32dd92a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-oluwatunmise/.conda/envs/team2/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/home/jupyter-oluwatunmise/.conda/envs/team2/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# ===== PCA-enabled trainer + safe PCA sweep + visuals =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "# ===== Labels (after renaming Pleural Effusion -> Effusion) =====\n",
    "\n",
    "label_cols = [\n",
    "    \"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\"Lung Lesion\",\"Edema\",\n",
    "    \"Consolidation\",\"Pneumonia\",\"Atelectasis\",\"Pneumothorax\",\"PleuralEffusion\",\n",
    "    \"Pleural Other\",\"Fracture\",\"Support Devices\"\n",
    "]\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def stack_from_samples(samples: List[Dict]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(samples) == 0:\n",
    "        raise ValueError(\"Empty sample list.\")\n",
    "    X_list, Y_list, ids = [], [], []\n",
    "    for s in samples:\n",
    "        emb = np.asarray(s[\"emb\"])\n",
    "        if emb.ndim > 1: emb = emb.reshape(-1)\n",
    "        X_list.append(emb.astype(np.float32))\n",
    "        Y_list.append(np.asarray(s[\"lab\"], dtype=np.float32))\n",
    "        ids.append(int(s[\"patient_id\"]))\n",
    "    X = np.vstack(X_list)\n",
    "    Y = np.vstack(Y_list)\n",
    "    ids = np.asarray(ids)\n",
    "    X = np.nan_to_num(X, posinf=0.0, neginf=0.0)\n",
    "    Y = np.nan_to_num(Y, posinf=0.0, neginf=0.0)\n",
    "    Y = (Y > 0.5).astype(int)\n",
    "    return X, Y, ids\n",
    "\n",
    "# ---------- trainer with optional PCA ----------\n",
    "def fit_eval_mlp_on_splits(\n",
    "    train_samples: List[Dict],\n",
    "    valid_samples: List[Dict],\n",
    "    test_samples:  List[Dict],\n",
    "    label_names: List[str],\n",
    "    seed: int = 0,\n",
    "    hidden=(512, 256),\n",
    "    max_iter=120,\n",
    "    pca_dim: int | None = None,   # NEW\n",
    "    pca_whiten: bool = False,     # optional\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    X_tr, Y_tr, _ = stack_from_samples(train_samples)\n",
    "    X_va, Y_va, _ = stack_from_samples(valid_samples)\n",
    "    X_te, Y_te, _ = stack_from_samples(test_samples)\n",
    "\n",
    "    # scale using TRAIN only\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = scaler.fit_transform(X_tr)\n",
    "    X_va_s = scaler.transform(X_va)\n",
    "    X_te_s = scaler.transform(X_te)\n",
    "\n",
    "    # PCA after concat (optional)\n",
    "    if pca_dim is not None and 0 < pca_dim < X_tr_s.shape[1]:\n",
    "        pca = PCA(n_components=pca_dim, whiten=pca_whiten, random_state=seed)\n",
    "        X_tr_s = pca.fit_transform(X_tr_s)\n",
    "        X_va_s = pca.transform(X_va_s)\n",
    "        X_te_s = pca.transform(X_te_s)\n",
    "\n",
    "    L = Y_tr.shape[1]\n",
    "    P_va = np.zeros((X_va_s.shape[0], L), dtype=float)\n",
    "    P_te = np.zeros((X_te_s.shape[0], L), dtype=float)\n",
    "\n",
    "    for j in range(L):\n",
    "        y_tr = Y_tr[:, j]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            const_p = float(y_tr.mean())\n",
    "            P_va[:, j] = const_p\n",
    "            P_te[:, j] = const_p\n",
    "            continue\n",
    "\n",
    "        sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden,\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=1e-4,\n",
    "            learning_rate_init=1e-3,\n",
    "            batch_size=256,\n",
    "            max_iter=max_iter,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=10,\n",
    "            validation_fraction=0.15,\n",
    "            shuffle=True,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        clf.fit(X_tr_s, y_tr, sample_weight=sw)\n",
    "        P_va[:, j] = clf.predict_proba(X_va_s)[:, 1]\n",
    "        P_te[:, j] = clf.predict_proba(X_te_s)[:, 1]\n",
    "\n",
    "    def split_metrics(Y_true: np.ndarray, P: np.ndarray) -> Dict[str, float]:\n",
    "        per_auc, per_ap, valid_cols = {}, {}, []\n",
    "        for j, lab in enumerate(label_names):\n",
    "            yt = Y_true[:, j]\n",
    "            if len(np.unique(yt)) < 2:\n",
    "                per_auc[lab] = np.nan\n",
    "                per_ap[lab]  = np.nan\n",
    "            else:\n",
    "                per_auc[lab] = roc_auc_score(yt, P[:, j])\n",
    "                per_ap[lab]  = average_precision_score(yt, P[:, j])\n",
    "                valid_cols.append(j)\n",
    "        auroc_macro = float(np.nanmean(list(per_auc.values()))) if per_auc else np.nan\n",
    "        ap_macro    = float(np.nanmean(list(per_ap.values())))  if per_ap else np.nan\n",
    "        if valid_cols:\n",
    "            auroc_micro = roc_auc_score(Y_true[:, valid_cols], P[:, valid_cols], average=\"micro\")\n",
    "            ap_micro    = average_precision_score(Y_true[:, valid_cols], P[:, valid_cols], average=\"micro\")\n",
    "        else:\n",
    "            auroc_micro = np.nan; ap_micro = np.nan\n",
    "        return dict(\n",
    "            AUROC_macro=auroc_macro, AP_macro=ap_macro,\n",
    "            AUROC_micro=auroc_micro, AP_micro=ap_micro,\n",
    "            per_label_AUROC=per_auc, per_label_AP=per_ap,\n",
    "        )\n",
    "\n",
    "    return {\"valid\": split_metrics(Y_va, P_va), \"test\": split_metrics(Y_te, P_te)}\n",
    "\n",
    "# ---------- SAFE PCA sweep over concatenated splits ----------\n",
    "# expects: concat_splits[\"train\"|\"valid\"|\"test\"] and label_cols already defined\n",
    "X_tr_concat, _, _ = stack_from_samples(concat_splits[\"train\"])\n",
    "full_dim = X_tr_concat.shape[1]\n",
    "\n",
    "# candidate dims; keep valid (< full_dim)\n",
    "pca_dims = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "pca_dims = [d for d in pca_dims if d < full_dim]\n",
    "if len(pca_dims) == 0:\n",
    "    print(f\"No PCA dims < concatenated dim ({full_dim}). Skipping PCA sweep.\")\n",
    "\n",
    "concat_grid_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n",
    "concat_grid_perlabel: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "for d in pca_dims:\n",
    "    tag = f\"PCA-{d}\"\n",
    "    try:\n",
    "        res_d = fit_eval_mlp_on_splits(\n",
    "            train_samples=concat_splits[\"train\"],\n",
    "            valid_samples=concat_splits[\"valid\"],\n",
    "            test_samples=concat_splits[\"test\"],\n",
    "            label_names=label_cols,\n",
    "            seed=0, hidden=(512,256), max_iter=120,\n",
    "            pca_dim=d,  # PCA after concat\n",
    "            # pca_whiten=True,  # optional\n",
    "        )\n",
    "        concat_grid_results[tag] = {\n",
    "            \"valid\": {k:v for k,v in res_d[\"valid\"].items() if not isinstance(v, dict)},\n",
    "            \"test\":  {k:v for k,v in res_d[\"test\"].items()  if not isinstance(v, dict)},\n",
    "        }\n",
    "        concat_grid_perlabel[tag] = res_d[\"test\"][\"per_label_AUROC\"]\n",
    "        print(f\"[{tag}] TEST AUROC_macro={concat_grid_results[tag]['test']['AUROC_macro']:.4f} \"\n",
    "              f\"| AP_macro={concat_grid_results[tag]['test']['AP_macro']:.4f} \"\n",
    "              f\"| AUROC_micro={concat_grid_results[tag]['test']['AUROC_micro']:.4f} \"\n",
    "              f\"| AP_micro={concat_grid_results[tag]['test']['AP_micro']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {tag} due to error: {e}\")\n",
    "\n",
    "# ---------- Plot: macro metrics vs PCA dim (robust to missing dims) ----------\n",
    "rows = []\n",
    "for d in pca_dims:\n",
    "    tag = f\"Concat-PCA-{d}\"\n",
    "    if tag not in concat_grid_results:\n",
    "        print(f\"⚠️ Skipping {tag} (no results)\")\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"pca_dim\": d,\n",
    "        \"AUROC_macro\": concat_grid_results[tag][\"test\"][\"AUROC_macro\"],\n",
    "        \"AP_macro\":    concat_grid_results[tag][\"test\"][\"AP_macro\"],\n",
    "        \"AUROC_micro\": concat_grid_results[tag][\"test\"][\"AUROC_micro\"],\n",
    "        \"AP_micro\":    concat_grid_results[tag][\"test\"][\"AP_micro\"],\n",
    "    })\n",
    "df_pca = pd.DataFrame(rows).sort_values(\"pca_dim\")\n",
    "\n",
    "if not df_pca.empty:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df_pca[\"pca_dim\"], df_pca[\"AUROC_macro\"], marker=\"o\")\n",
    "    plt.xlabel(\"PCA dimension (after concat)\")\n",
    "    plt.ylabel(\"Test AUROC (macro)\")\n",
    "    plt.title(\"Concat → PCA: Test AUROC_macro vs. PCA dim\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df_pca[\"pca_dim\"], df_pca[\"AP_macro\"], marker=\"o\")\n",
    "    plt.xlabel(\"PCA dimension (after concat)\")\n",
    "    plt.ylabel(\"Test AP (macro)\")\n",
    "    plt.title(\"Concat → PCA: Test AP_macro vs. PCA dim\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No PCA points to plot.\")\n",
    "\n",
    "# ---------- Optional: per-label heatmap over PCA variants ----------\n",
    "def plot_per_label_heatmap(per_label_results: Dict[str, Dict[str, float]],\n",
    "                           label_cols: List[str],\n",
    "                           title=\"Per-label AUROC: Concat + PCA sweep\"):\n",
    "    df_scores = pd.DataFrame(per_label_results).T\n",
    "    for c in label_cols:\n",
    "        if c not in df_scores.columns:\n",
    "            df_scores[c] = np.nan\n",
    "    df_scores = df_scores[label_cols]\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(df_scores, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "                cbar_kws={'label':'AUROC'})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Disease Label\")\n",
    "    plt.ylabel(\"Model Variant\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(concat_grid_perlabel):\n",
    "    plot_per_label_heatmap(concat_grid_perlabel, label_cols,\n",
    "                           title=\"Per-label AUROC — Concat with PCA (various dims)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84083b2-63a5-4a75-8468-34e9675a11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "def plot_per_label_heatmap_ranked(\n",
    "    per_label_results: Dict[str, Dict[str, float]],\n",
    "    label_cols: List[str],\n",
    "    title: str = \"Per-label AUROC (bright = best, includes Average)\",\n",
    "    add_average: bool = True,\n",
    "    sort_rows_by_avg: bool = False,\n",
    "    star_char: str = \"★\",\n",
    "    star_tol: float = 1e-12,   # treat near-equal as ties\n",
    "):\n",
    "    # 1) Wide DF of true AUROCs (rows = models/variants, cols = labels)\n",
    "    df_true = pd.DataFrame(per_label_results).T\n",
    "    for c in label_cols:\n",
    "        if c not in df_true.columns:\n",
    "            df_true[c] = np.nan\n",
    "    df_true = df_true[label_cols]\n",
    "\n",
    "    # 2) Average column\n",
    "    if add_average:\n",
    "        df_true[\"Average\"] = df_true[label_cols].mean(axis=1)\n",
    "\n",
    "    # 3) Optional sort by Average (desc)\n",
    "    if sort_rows_by_avg and \"Average\" in df_true.columns:\n",
    "        df_true = df_true.sort_values(\"Average\", ascending=False)\n",
    "\n",
    "    # 4) Column-wise normalization → color brightness shows rank per label\n",
    "    df_norm = df_true.copy()\n",
    "    for c in label_cols:\n",
    "        col = df_norm[c].to_numpy(dtype=float)\n",
    "        cmin, cmax = np.nanmin(col), np.nanmax(col)\n",
    "        if np.isfinite(cmin) and np.isfinite(cmax) and cmax > cmin:\n",
    "            df_norm[c] = (col - cmin) / (cmax - cmin)\n",
    "        else:\n",
    "            df_norm[c] = 0.5  # flat/undefined → neutral brightness\n",
    "\n",
    "    # Normalize Average column for color, too (optional but looks nicer)\n",
    "    if \"Average\" in df_norm.columns:\n",
    "        av = df_true[\"Average\"].to_numpy(dtype=float)\n",
    "        if np.isfinite(av).all() and np.nanmax(av) > np.nanmin(av):\n",
    "            df_norm[\"Average\"] = (av - np.nanmin(av)) / (np.nanmax(av) - np.nanmin(av))\n",
    "        else:\n",
    "            df_norm[\"Average\"] = 0.5\n",
    "\n",
    "    # 5) Build annotation text with stars on per-label winners\n",
    "    annot_text = df_true.copy().astype(float).applymap(lambda v: \"\" if np.isnan(v) else f\"{v:.2f}\")\n",
    "\n",
    "    for c in label_cols:\n",
    "        col_vals = df_true[c].astype(float)\n",
    "        if col_vals.notna().any():\n",
    "            mx = col_vals.max()\n",
    "            # treat near-equal within star_tol as ties\n",
    "            winners = col_vals.index[(np.abs(col_vals - mx) <= star_tol)]\n",
    "            for idx in winners:\n",
    "                # append star to existing formatted value\n",
    "                annot_text.loc[idx, c] = f\"{annot_text.loc[idx, c]}{star_char}\"\n",
    "\n",
    "    # 6) Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.heatmap(\n",
    "        df_norm, annot=annot_text, fmt=\"\", cmap=\"viridis\",\n",
    "        cbar_kws={'label': 'Relative (per-label) performance'},\n",
    "        vmin=0.0, vmax=1.0\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Disease Label\" + (\" + Average\" if \"Average\" in df_true.columns else \"\"))\n",
    "    ax.set_ylabel(\"Model / Variant\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "##Run\n",
    "    \n",
    "plot_per_label_heatmap_ranked(\n",
    "    concat_grid_perlabel,   # e.g., {\"Concat-PCA-16\": {\"Atelectasis\":0.84, ...}, ...}\n",
    "    label_cols,\n",
    "    title=\"Per-label AUROC — Concat + PCA (dims 4-> 1024; column-normalized, best=bright)\",\n",
    "    add_average=True,\n",
    "    sort_rows_by_avg=False   # keep rows ordered by PCA dim name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team2",
   "language": "python",
   "name": "team2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
